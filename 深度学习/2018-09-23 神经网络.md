## 万能近似定理
《深度学习》书籍170页
## 神经元
```mathjax
 y = \omega x+b
```
w（weight） 一般叫做 权重，weight 英文翻译：加权、重要性、分量、影响、衡制
b （bias）一般叫做 偏置，bias 英文翻译：倾向、偏见、偏差
> B站 3BlueBrown 深度学习网络结构第 11 分钟

```mathjax
y = \sigma (\omega _1 x_1+\omega _2 x_2+\omega _3 x_3+\omega _4 x_4+b) \quad ( \sigma 是 activate function 激活函数)
```
举例：y = 5x-20, 激活函数是 Sigmoid
```mathjax
\begin{bmatrix}
	2  \\
	3 \\ 
	5  \\
	6  \\
\end{bmatrix}
=> y =>
\begin{bmatrix}
	-10  \\
	-5 \\ 
	5  \\
	10  \\
\end{bmatrix}
=> \sigma =>
\begin{bmatrix}
	0.00004539  \\
	0.00669285 \\  
	0.99330714  \\
	0.99995460  \\
\end{bmatrix}
```
### 神经元如何处理不均匀分布数据，w和b的作用到底是什么
案例：
假如训练样本输入房价数据范围是 10000 - 20000，而且分布不均匀，50% 数据分布在 10000 - 18000，是平民房，label = 0， 剩下一半集中在 18000 - 20000，18000以上的我们叫高端房，label = 1，求输入一个房价它是高端房的概率是多少？
```mathjax
y = \sigma (\omega  x+b) \quad ( \sigma 是 Sigmoid 激活函数)
```
分析：这是一个很简单的 2 分类问题，我们要学习出w和b，将 label 0 和 1 完全区分开，最理想的情况下 当对以下模型输入 18000时，输出应该是0.5，18000 以下的小于0.5，18000 以上的大于0.5.
问题1: 那到底这个w和b应该是多少呢？
![](/数学/_image/2018-09-14-15-43-30.jpg)
根据 Sigmoid 函数图像，我们可以知道$\sigma (0)=0.5$，那么：
```mathjax
要求：输入 x = 18000 ，输出 y = 0.5\\
\sigma (0)=0.5 \\ 
=> w \cdot 18000+b=0
```
满足条件的w、b有无数种组合，到底该选哪一种？
比如：

w = 1, b = -18000, 对应输入就变为了[10000,20000]
w = 0.1, b = -1800,对应输入就变为了[1000,2000]
w = 0.01, b = -180,对应输入就变为了[100,200]
w = 0.001, b = -18,对应输入就变为了[10,20]
w = 0.0001, b = -1.8,对应输入就变为了[1,2]

通过这个我们可以看到，w 的作用是用来缩放区间，而偏置 b的作用则是找到那个合理的分割点，b 我更愿意称它为阀值。
 
对 这几种 w、b 输入测试数据看看，两个测试数据为17000，19000:
```table
|w、b|wx+b  (17000\19000)|Sigmoid(wx+b) |
w = 1, b = -18000|-1000\1000|约0.00000000e+00\约1
w = 0.1, b = -1800|-100\100|3.72007598e-44\约1
w = 0.01, b = -180|-10\10|0.00004.53978\0.9999546
w = 0.001, b = -18|-1\1|0.268941421\0.73105858
w = 0.0001, b = -1.8|-0.1\0.1|0.475020813\0.52497919
```
对比我们能发现其实 w = 0.0001, b = -1.8，这一对数据可能会更好些，更能反应实际的情况。这也跟 Sigmoid 函数图像对得上，Sigmoid 函数在 [-5,+5] 区间内梯度明显，效果最好，超过这个范围后，基本都平滑了。这就是 Sigmoid 函数存在的“梯度消失问题”。在神经网络中如果wb参数初始化的较大的时候，会出现梯度消失，几乎不再收敛了。

问题2:
这个 w = 0.0001, b = -1.8 到底是不是最优解？如何得到唯一最优解的w、b？
答：在逻辑回归中可以使用损失函数来估计参数w和b，神经网络中常使用交叉熵或平方差。
交叉熵是使用极大似然估计法来求最终值。这里看《数学-极大似然估计》了解详细。这里假设训练数据为如下：
（题目假设了训练样本输入房价数据范围是 10000 - 20000，而且分布不均匀，50% 数据分布在 10000 - 18000，是平民房，label = 0， 剩下一半集中在 18000 - 20000，18000以上的我们叫高端房，label = 1）
这里我们用6个训练样本
```table
|房价|label|
11000|0
15000|0
17000|0
18500|1
19000|1
19500|1
```

## 激活函数
### 激活函数的作用
激活函数类似生物学中的“神经元”，提供一个输入到输出的映射，我理解的生物中神经元存在激活或不激活两种状态，激活时有激活的强弱。所以激活函数应该提供这样一种能力，即：类似表示激活与非激活状态的简单“二分类”能力，激活状态有激活的强弱。这样一看好像ReLu函数真的很合适。
### 激活函数的性质
性质分为两种，一种是根据生物学模拟需要的性质，一种是为了计算方便需要的性质。
生物学模拟需要的性质：
* 非线性：神经元是线性的，引入激活函数的非线性，使得神经网络可以拟合任意的线性和非线性（怎么理解？）
计算方便需要的性质：求导方便
* ReLu  求导快速

## 梯度下降


