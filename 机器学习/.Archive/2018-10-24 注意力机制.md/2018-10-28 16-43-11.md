## 问题
1. 注意力权重，即概率分布如何计算
2. 加权求和如何做
3. 怎么在 RNN 中实现
4. 代码实现
## 参考资料
根据这篇文章基本看懂了
[深度学习中的注意力机制](https://blog.csdn.net/qq_40027052/article/details/78421155)
[](https://www.cnblogs.com/robert-dlut/p/5952032.html)
## 需注意
1. 正确理解 RNN 结构很重要，最主要看图一定要理解其关于时间轴展开的形式，如下图，不然很容易陷入误区，不知所云。
![](./_image/2018-10-24-07-53-32.jpg?r=44)
2. 最好根据编码解码模型去理解
## 问题理解
1. 注意力权重，即概率分布如何计算
![](./_image/2018-10-24-07-54-43.jpg?r=66)
对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即**通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性**，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。
2. 注意力权重怎么用
[深度学习中的注意力机制](https://blog.csdn.net/qq_40027052/article/details/78421155)
![](./_image/2018-10-24-08-01-00.jpg)
![](./_image/2018-10-24-08-01-11.jpg?r=87)
![](./_image/2018-10-24-08-01-21.jpg)
![](./_image/2018-10-24-07-59-52.jpg?r=72)
## 用途
注意力机制很适合 序列到序列 的问题解决。
如果Source是中文句子，Target是英文句子，那么这就是解决**机器翻译问题**的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是**文本摘要**的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是**问答系统**或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。
Encoder-Decoder框架不仅仅在**文本领域**广泛使用，在**语音识别、图像处理等领域**也经常使用。比如对于**语音识别**来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“**图像描述**”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。
