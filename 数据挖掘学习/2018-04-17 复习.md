# 数据挖掘基本流程
## 需求转化为可执行的问题定义
普通卡转会员卡预测。样本取什么，label取什么？正样本是啥，负样本是啥，怎么划分数据集。
## 需求分析
问题本质就是：什么样的人什么样的时间在什么样的地方买了什么样的东西。
根据对问题的本质的理解去看能取到什么数据。
##数据提取、
##分析、
##清洗
1. 异常值处理
    如何看出异常值来？散点图，正态分布又叫高斯分布。
2. 空值处理
    * 填补：平均数、众数
    * 把空值当作一类单独处理
## 提取特征
特征提取思路，取决于对问题和数据的分析和了解，我觉得很重要，直接决定了你对问题的定义，以及会不会漏掉重要特征。比如年卡这个，问题本质就是：什么样的人什么样的时间在什么样的地方买了什么样的东西。
然后接下来就按四条主线去提取特征就没错，什么样的人、什么地方、买了什么东西
什么样的人，先说他个人长期的消费特征，再说她的消费水平在整个社区是什么样的水平，还有她最近的消费特征
等。
什么样的店，人流、销售额、销售单数等，反映了这家店的画像
人和店，一人对多店，取均值
取人最近30天的特征后，提升效果明显，应该是过滤掉了很多最近不常消费的睡眠户。
数据集划分，正样本和负样本的划分
## 特征工程
### 数据标准化
连续的值：
    * 无量纲化，或者叫归一化，将数据缩放到同一个区间，这样可以让不同量级和单位的数据能比较和加权，为什么？能提高模型的收敛速度，使不同维度的特征在数值上有一定的比较性，能提高分类器的准确率。
        * 归一化方法：min-max方法，（x-min）/max-min，数据分布在0到1之间。z-sore方法，又叫标准差标准化，x-均值/标准差，数据分布在-1到1之间。适用于数据是正态分布的情况，经过处理后的数据均值为0，标准差是1.标准差是方差的算术平方根。
    * band处理，将连续值变为离散不连续的区间，比如泰坦尼克号中的age，划分为年龄段处理。
离散的类别值：
    * one-hot编码：为什么要独热编码，比如几种颜色值用 123456 来表示，在欧式空间向量距离不等，提供了一个潜在的大小排序，会影响输出结果。模型处理时会认为他们之间有一定连续型关系，实际上并没有，逻辑回归会这样，随机森林这种树模型不需要one-hot。表示成0001等，增加了特征维度，类别多的话会造成纬度爆炸。
为了避免纬度爆炸可以怎么做，可以结合PCA特征降维方法。

## 特征选择
### 特征降维
PCA：主成分分析法，将高纬空间的数据正交投影到低纬子空间，使投影空间的方差最大化，这样可以尽量保证特征的主要信息。
好处是：可以降维，模型的储存和分类性能更高了，一定程度上提高了模型的泛化能力，
缺点是：可能会丢失些类别信息。
### 可以使用随机森林等树模型算法，给特征重要性排序，选取重要性高的特征

## 模型训练
生成模型、判别模型
分类、回归、聚类
输出是有限离散个的是分类问题，输入输出都是连续变量的是回归问题，没有label的输入输出都是变量的，那种无监督学习叫聚类。
### K-mean 是聚类算法，K-NN（K近邻）是分类回归算法
### 逻辑回归
### SVM 支持向量机
### 决策树
### Random Forest 随机森林
由完全生长的二叉树构成，集成学习方法是bagging，分类是多数投票，回归是简单平均
优点：可解释性强、可处理混合类型的特征、可自然的处理缺失值，对异常点鲁棒性好。尤特好着呢个选择作用，容易并行、3个随机项（1.取样方法是bootstrap有放回取样，随机取n样本2、训练t棵树3.从总特征中选择k个特征组合）的引入，使它不容易过拟合
对于回归树，平方误差最小化原则，对于分类树，基尼指数最小化原则。
### 朴素贝叶斯
基于贝叶斯定理与特征条件独立假设。
优点：高效、易于实现
缺点：分类性能不一定很高。
### adaBoost
传统Boosting模型，每次训练会改变样本的权重，使上一次被错分的样本权重变大，得到更大的关注来改善分类性能，最后结果是多次迭代后权重累计的结果
### ？GDBT
与传统 Boosting 模型最大的区别是，每次迭代是为了减少上一次的残差，关键就是利用损失函数的负梯度方向的值作为残差的近似值。与传统的 Boosting 中关注正确错误样本的加权有很大的区别。
GBDT 会累加所有树的结果，这种累加是无法通过分类产生的，所以GDBT的树都是CART回归树，不是分类树
优点：比RF基础上又有了进一步提升，能灵活处理各种类型的数据，预测准确度较高
缺点：它是 Boosting ，所以基学习器之间存在串行关系，难以并行训练数据
###？XGBoost
是什么：比 GBDT  又有了进一步提升，性能高，能自动运用CPU的多线程并行运算，精度也有提高
优点：
1. 传统 GBDT 以 CART 树作为基分类器，XGBoost 还支持线性分类器，这个时候相当于 L1 和 L2 正则化的逻辑回归。
2. 列抽样，借鉴了随机森林，支持列抽样，不仅能防止过拟合，还能减少计算 
3. 缺失值处理，对于缺失值可自动学习出它的分裂方向。
4. 最重要的**支持并行**。
###？集成学习方法
Bagging：代表性是 随机森林，有放回取样，多数投票（分类），简单平均（回归）
Boosting：adaBoost（每次迭代改变样本权重）、GBDT（每次迭代为了减少上一次迭代的残差）、XGBoost（）
# ？深度学习
# ？TensorFlow

## 交叉验证和模型选择
模型选择、调参
## 欠拟合和过拟合处理
判断，学习曲线
过拟合：增加训练集、减少特征维度、正则化（结构风险最小化，权重的惩罚项，使方差更小，增加泛化能力，使曲线变得平滑，L1正则是权重向量各个元素的绝对值之和，L2正则是权重向量各个元素的平方和再求平方根，L2正则回归也叫岭回归）
欠拟合：增加训练集、
## 评估
Accuracy = 预测值/总样本数，易受不均衡数据影响，
Presicion = 预测值中预测对了的概率
Recall = 所有正样本中被预测对了的概率
F1值：Precision 和 Recall 的调和平均数，会综合考虑P和R
AUC：是ROC曲线下的面积值，对不均衡数据有很好的评估效果，怎么理解呢，随机给分类器一个正样本和一个负样本，分类器输出正样本为正的概率比输出负样本为正的概率大的概率。
ROC曲线和P-R曲线
横坐标是假阳率，纵坐标是真阳率，越往左上越好。P-R是越往右上越好。
优点：不受数据均衡与否的影响，正负样本分布发生变化时，ROC曲线保持不变。
## 预测


# 文本处理
关键词过滤，词性色彩处理


# ？MapReduce

面试时说：随机森林、支持向量机、逻辑回归、K-mean等








