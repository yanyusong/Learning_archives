哲学家康德说：“真正的自由不是随心所欲，而是自我主宰。自律即自由
## 信息增益与熵
信息增益指两个熵之间的差值。
## 树状结构
![](./_image/2018-04-08-20-41-34.jpg?r=33)
在树状结构中的基本单位，称为节点（Node）。节点之间的连结，称为分支（branch）。节点与分支形成树状，结构的开端，称为根（root），或根结点。根节点之外的节点，称为子节点（child）。没有连结到其他子节点的节点，称为叶节点（Leaf）。
### 树模型的优缺点
![](./_image/2018-04-09-22-26-39.jpg?r=80)
- - - - -
## 决策树
[决策树](https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91)
### what
分类和回归
### 决策树学习
决策树生成算法：ID3算法（以信息增益），C4.5算法（信息增益比），CART算法
三个步骤：特征选择、决策树生成、决策树的修剪
最常见的树是 CART 树，分类 And 回归 树，分类回归树，是二叉树
#### CART树生成
CART 的生成就是递归的构建二叉决策树的过程。对于回归树用**平方误差最小化准则**；对于分类树用**基尼指数最小化准则**。（Gini指数）
传统 CART树寻找最优切分点的标准是最小化均方差。
对于连续值找到使样本集平方误差最小的切分变量和切分变量的切分值，依此将输入空间划分为两个区域。重复以上步骤就生成了一棵回归树。
对于分类树，用基尼指数来确定该特征的最优二值分割点。基尼指数具体推倒忘记了，可以理解为，基尼指数可以表示样本集合的不确定性。基尼指数值越大，样本集合的不确定性也越大。能使基尼指数最小的特征和值，也就是说能使样本不确定性减少最多的，就是当前情况下的最优切分特征和最优切分点。
关键词：选择最优特征、选择最优分割点、Gini指数、熵、信息增益、信息增益比
#### CART 剪枝
预剪枝、后剪枝
首先从生成的决策树的最底端开始不断剪枝，直到根结点，形成一个子树序列。然后通过交叉验证在独立的验证数据集上对子序列进行测试，从中选择最优子树。
### 优点
### 缺点
容易过拟合，剪枝，使用随机森林可减少过拟合
###相关条目
贝叶斯定理
贝叶斯概率
马尔科夫链
- - - - -
## 最优化方法
梯度下降和牛顿法
### 梯度下降
梯度下降是最小化目标函数的一种优化方法。简单来说就是找到目标函数下降最快的地方。类似于下山一样。
缺点：目标函数如果是凸函数，收敛后就是全局最优解，如果不是则可能陷入局部最优解，不一定是全局最优解。
学习速率的步长太大会不收敛，在极点处震荡；太小则收敛迭代速度太慢。
### 牛顿法
## Bootstrap 取样法
也叫 “自助取样法” ，即有放回取样。重复多次后，极限会有 36.8% 的数据不会被取到，这些可以被用于测试。
## 为什么正则化，L1和L2正则化区别，什么时候用
