## 语言表示
语言表示是所有 NLP 模型的基础。
[不是你无法入门自然语言处理（NLP），而是你没找到正确的打开方式](http://m.sohu.com/a/142920778_717210)
[词向量经典模型：从word2vec、glove、ELMo到BERT](https://zhuanlan.zhihu.com/p/51682879)
[Hello NLP(1)——词向量Why&How](https://zhuanlan.zhihu.com/p/51040131)
[词嵌入(word2vec)-NNLM（Neural Network Language Model）](https://blog.csdn.net/qq_39422642/article/details/78658309)
[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)
[[NLP自然语言处理]谷歌BERT模型深度解析](https://blog.csdn.net/qq_39521554/article/details/83062188)
## 语言模型
词袋模型 bag of word（one-hot，）
n-gram 语言模型（使用前边的n-1个词，预测第n个词，概率图模型的基础）
神经网络语言模型 NNLM （word2vec，在n-gram基础上，cbow 使用 左右预测中间词，skip-gram 使用中间词预测周边词）
### 方法概览
Word Embedding 存在的问题
多语义问题，比如：Bank 银行 / 河岸。
### 词的表示
词嵌入
![](./_image/2018-12-07-17-24-00.jpg?r=70)
one-hot 表示，
分布式表示，包含上下文信息，又叫词嵌入
![](_image/2018-06-20-14-49-54.jpg)
![](_image/2018-06-20-14-51-57.jpg)
## 词向量模型 word2vec
[word2vec 最好的论文 ：word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)
[Deep Learning in NLP （一）词向量和语言模型](http://licstar.net/archives/328)
[Chinese Word Vectors：目前最全的中文预训练词向量集合](https://mp.weixin.qq.com/s/tENiB4P1--sD5B5r3Af16w)
### 词向量是什么
[[NLP] 秒懂词向量Word2vec的本质](https://zhuanlan.zhihu.com/p/26306795)
### 词向量生成
[知乎 word2vec是如何得到词向量的？](https://www.zhihu.com/question/44832436)
Word2Vec 包含两种词训练模型，CBOW模型 和 Skip-gram 模型。
* CBOW 模型根据中心词 W(t) 周围的词来预测中心词
* Skip-gram 模型根据中心词 W(t) 来预测周围词
    [Word2Vec教程 - Skip-Gram模型](https://blog.csdn.net/layumi1993/article/details/72866235)
所有的词向量由 ngram2vec 工具包训练。ngram2vec 工具包是 word2vec 和 fasttext 工具包的超集合，其支持抽象上下文特征和模型。
* ngram2vec：https://github.com/zhezhaoa/ngram2vec/
* word2vec：https://github.com/svn2github/word2vec
* fasttext：https://github.com/facebookresearch/fastText
### 词向量质量评估
词向量的质量通常由类比问题任务进行评估
###
GloVe、ELMo（Embeddings from Language Models） 、Bert