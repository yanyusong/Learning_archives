## 基本思想
CBOW （Continuous Bag Of Word）
Skip-gram
![](./_image/2019-01-08-11-30-55.jpg)
## Word2Vec
![](./_image/2019-01-07-19-25-23.jpg)
![](./_image/2019-01-08-14-50-28.jpg)
### 系列
[刘建平 Word2Vec系列超级棒](https://www.cnblogs.com/pinard/category/894695.html)
[刘建平 word2vec 负采样 Negative Sampling](https://www.cnblogs.com/pinard/p/7249903.html)
[理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)
![](./_image/2019-01-08-15-01-33.jpg)
### 问题
传统神经网络结构是啥？
Word2Vec的改进？
输入输出是啥？
损失函数是啥？
收敛条件是啥？怎么证明收敛性？
### 问题解答
#### 传统神经网络结构是啥？
![](./_image/2019-01-08-15-31-58.jpg)
缺点：V是词汇表的大小，隐藏层到输出层计算量超级大。
#### Word2Vec的改进、 输入输出是啥？
输入层到隐藏层：采用简单的对所有输入词向量求和并取平均的方法
用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。
（Negative sampling）<?>
#### 层次Softmax Hierarchical Softmax 在word2vec 中怎么应用的？
![](./_image/2019-01-08-15-41-31.jpg)
逐层 Sigmoid，
如何“沿着霍夫曼树一步步完成”呢？在word2vec中，采用了二元逻辑回归的方法，即规定沿着左子树走，是负类(霍夫曼树编码1)，沿着右子树走，是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：
```mathjax
P(+) = \sigma(x_w^T\theta) = \frac{1}{1+e^{-x_w^T\theta}}\\
P(-) =  1-P(+)
```
对于上图中的w2，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点 n(w2,1) 的P(−)概率大，n(w2,2)的P(−)概率大，n(w2,3)的P(+)概率大。
####<?> 损失函数是啥，训练过程 <?>
使用最大似然法来寻找所有节点的词向量和所有内部节点θ。以上面的 w2 例子来看，我们期望最大化下面的似然函数：
```mathjax
\prod_{i=1}^3P(n(w_i),i) = (1- \frac{1}{1+e^{-x_w^T\theta_1}})(1- \frac{1}{1+e^{-x_w^T\theta_2}})\frac{1}{1+e^{-x_w^T\theta_3}}
```
#### 随机梯度上升
跟梯度下降一样，只不过把减换成加。
#### 收敛条件是啥？怎么证明收敛性？
《凸优化理论》凸函数可以保证全局最优，不是凸函数的话也可以收敛，不过可能是局部最优解，不保证是全局最优解。
#### Negative Sampling 
![](./_image/2019-02-06-20-12-36.jpg)
负采样方法
![](./_image/2019-02-06-20-15-30.jpg)
## FastText
FastText 专注于文本分类。
fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。
也是huffman 树，只不过预测的是类别。向量表示添加了 n-gram 的特性。
fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。




